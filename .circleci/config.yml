version: 2.1

orbs:
  docker: circleci/docker@2.1.2
  aws-eks: circleci/aws-eks@2.2.0
  kubernetes: circleci/kubernetes@1.3

commands:
  # Command to install awscli and gettext-base
  install-awscli-gettext:
    description: Install awscli and gettext-base
    steps:
      - run:
          name: Install awscli and gettext-base
          command: |
            sudo apt update
            sudo apt-get install awscli
            sudo apt-get install gettext-base

  # Command to install aws-iam-authenticator
  install-aws-iam-authenticator:
    description: Install aws-iam-authenticator
    steps:
      - run:
          name: Install aws-iam-authenticator
          command: |
            curl -o aws-iam-authenticator curl -o aws-iam-authenticator \
              https://amazon-eks.s3-us-west-2.amazonaws.com/1.13.7/2019-06-11/bin/linux/amd64/aws-iam-authenticator
            chmod +x ./aws-iam-authenticator
            sudo mv ./aws-iam-authenticator /usr/local/bin/aws-iam-authenticator

  # Command to install kubectl
  install-kubectl:
    description: Install kubectl
    steps:
      - run:
          name: Install kubectl
          command: |
            # Download the kubectl binary from the specified URL using curl
            curl -o kubectl https://amazon-eks.s3-us-west-2.amazonaws.com/1.13.7/2019-06-11/bin/linux/amd64/kubectl

            # Add execute permission (+x) to the downloaded kubectl binary
            chmod +x ./kubectl

            # Move the kubectl binary to the /usr/local/bin directory
            # Note: This requires administrative privileges, hence 'sudo' is used.
            sudo mv ./kubectl /usr/local/bin/kubectl

jobs:
  # Job to run linting
  run-lint:
    docker:
      - image: cimg/python:3.7.13
    steps:
      - checkout
      # Restore cached dependencies based on the checksum of "app/requirements.txt"
      # The cached dependencies were saved with the key "v1-dependencies-{{ checksum "app/requirements.txt" }}"
      # If a matching cache is found, it will be restored, otherwise, it will fallback to a more general cache key.
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "app/requirements.txt" }}

            - v1-dependencies-
      - run:
          name: Install dependencies
          command: |
            chmod -R +x ./bin/
            python3 -m venv .devops-capstone
            source .devops-capstone/bin/activate
            make install
      - save_cache:
          paths:
            - ./.devops-capstone
          key: v1-dependencies-{{ checksum "app/requirements.txt" }}
      - run:
          name: Run lint
          command: |
            source .devops-capstone/bin/activate
            make lint

  # Job to build and push Docker image to Docker Hub
  build-and-push-docker-image-to-docker-hub:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - setup_remote_docker:
          version: 19.03.13
      - run:
          name: Build and push docker image
          command: |
            export VERSION=$(<app/version.txt)

            # Build docker image udacity-capstone-project
            docker build -t udacity-capstone-project app

            # Make tag for docker image udacity-capstone-project
            docker tag udacity-capstone-project ${DOCKER_HUB_ID}/${DOCKER_REPOSITORY}:${VERSION}
            docker tag udacity-capstone-project ${DOCKER_HUB_ID}/${DOCKER_REPOSITORY}:lastest

            # Ensure we're logged in to Docker Hub and push the images
            docker login -u $DOCKER_LOGIN -p $DOCKER_PASSWORD

            # I push docker image with two tags:
            # - "version" - version from file "app/version.txt"
            # - "latest" - the latest version
            docker push ${DOCKER_HUB_ID}/${DOCKER_REPOSITORY}:${VERSION}
            docker push ${DOCKER_HUB_ID}/${DOCKER_REPOSITORY}:lastest

  # Job to deploy the green version
  deploy-green:
    docker:
      - image: cimg/python:3.7.13
    steps:
      - checkout
      - install-awscli-gettext
      - install-aws-iam-authenticator
      - install-kubectl
      - run:
          name: Connect to cluster
          command: |
            aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $CLUSTER_NAME
      - run:
          name: Deploy new version as green
          command: |
            # Get the version from file "app/version.txt"
            export VERSION=$(<app/version.txt)
            # Get the Docker Hub ID from environment variable "DOCKER_HUB_ID"
            export DOCKER_IMAGE=${DOCKER_HUB_ID}/${DOCKER_REPOSITORY}:${VERSION}
            # Get the version from file "app/version.txt" and replace "." with "-"
            export LABEL_VERSION=${VERSION/./-}

            # Print the variables
            echo "DOCKER_IMAGE: $DOCKER_IMAGE"

            # Print LABEL_VERSION
            echo "LABEL_VERSION: $LABEL_VERSION"

            # Print Docker Hub ID
            echo "DOCKER_HUB_ID: $DOCKER_HUB_ID"
            echo "DOCKER_REPOSITORY: $DOCKER_REPOSITORY"

            # Apply environment variable substitution to the k8s/deployment.yaml file
            # using 'envsubst', and then apply the modified YAML to the Kubernetes cluster.
            envsubst < k8s/deployment.yaml | kubectl apply --filename -


            # Apply environment variable substitution to the k8s/service-green.yaml file
            # using 'envsubst', and then apply the modified YAML to the Kubernetes cluster.
            envsubst < k8s/service-green.yaml | kubectl apply --filename -



            # Wait for 3 seconds to allow the changes to be fully applied before checking the load balancer's hostname.

            # Get the hostname of the load balancer associated with the 'udacity-capstone-project-green' service
            # using 'kubectl get services' with a custom JSONPath expression.
            # The hostname is then assigned to the 'LOAD_BALANCER' variable.
            sleep 3
            LOAD_BALANCER=$(kubectl get services \
              udacity-capstone-project-green \
              --output jsonpath='{.status.loadBalancer.ingress[0].hostname}')

            # Print the hostname of the load balancer to the console.
            echo "LOAD_BALANCER: $LOAD_BALANCER"

  # Job to need for manual approval
  need-manual-approval:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - run: echo "A hold job to wait for a manual approval to make deploy as new blue"

  # Job to deploy the new blue version
  deploy-new-blue:
    docker:
      - image: cimg/python:3.7.13
    steps:
      - checkout
      - install-awscli-gettext
      - install-aws-iam-authenticator
      - install-kubectl
      - run:
          name: Connect to cluster
          command: |
            # Connect to cluster using the AWS CLI
            aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $CLUSTER_NAME
      - run:
          name: Target new version â€” new blue
          command: |
            # Read the content of the "app/version.txt" file and assign it to the "VERSION" variable
            export VERSION=$(<app/version.txt)

            # Replace any occurrences of dots (.) in the "VERSION" variable with dashes (-)
            # For example, if VERSION="1.2.3", LABEL_VERSION will become "1-2-3"
            export LABEL_VERSION=${VERSION/./-}

            # Print the "LABEL_VERSION" variable to the console
            echo "LABEL_VERSION: $LABEL_VERSION"


            # Apply environment variable substitution to the k8s/service.yaml file
            # using 'envsubst', and then apply the modified YAML to the Kubernetes cluster.
            # This step is likely used to update the service's configuration with the new version label.
            envsubst < k8s/service.yaml | kubectl apply --filename -

  # Job to remove the old blue version
  remove-old-blue:
    docker:
      - image: cimg/python:3.7.13
    steps:
      - checkout
      - install-awscli-gettext
      - install-aws-iam-authenticator
      - install-kubectl
      - run:
          name: Connect to cluster
          command: |
            # Connect to cluster using the AWS CLI and kubectl
            aws eks --region $AWS_DEFAULT_REGION update-kubeconfig --name $CLUSTER_NAME
      - run:
          name: Remove the old version udacity-capstone-project
          command: |
            # Read the content of the "app/version.txt" file and assign it to the "VERSION" variable
            export VERSION=$(<app/version.txt)

            # Replace any occurrences of dots (.) in the "VERSION" variable with dashes (-)
            # For example, if VERSION="1.2.3", LABEL_VERSION will become "1-2-3"
            export LABEL_VERSION=${VERSION/./-}

            # Print the "LABEL_VERSION" variable to the console
            echo "LABEL_VERSION: $LABEL_VERSION"

            # Remove the old deployments of the udacity-capstone-project except the one with the new label version
            # The for loop iterates over each deployment returned by "kubectl get deployments"
            # If the deployment name is not equal to the expected deployment name with the new label version,
            # then delete that deployment.
            for deploy in $(kubectl get deployments -o jsonpath="{.items[*].metadata.name}" | grep udacity-capstone-project);
            do
              if [[ $deploy != udacity-capstone-project-$LABEL_VERSION ]];
              then
                kubectl delete deployments $deploy
              fi
            done

            # Remove the udacity-capstone-project-green service if it exists
            # This is done using the 'kubectl get services' command and checking if the output contains the service name.
            if kubectl get services | grep udacity-capstone-project-green;
            then
              kubectl delete services udacity-capstone-project-green
            fi

workflows:
  default:
    jobs:
      - run-lint
      - build-and-push-docker-image-to-docker-hub:
          requires: [run-lint]
      - deploy-green:
          requires: [build-and-push-docker-image-to-docker-hub]
      - need-manual-approval:
          type: approval
          requires: [deploy-green]
      - deploy-new-blue:
          requires: [need-manual-approval]
      - remove-old-blue:
          requires: [deploy-new-blue]
